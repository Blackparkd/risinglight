\documentclass[a4paper,12pt]{scrreprt}

% Pacotes essenciais
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{float}
\usepackage{pdflscape}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{amsmath}
\usepackage{array}
\usepackage{caption}
\usepackage{multirow}
\usepackage{hyperref}
\usepackage[intoc]{nomencl}
\makenomenclature
\usepackage{tikz}
\usetikzlibrary{calc}

\usepackage{arydshln}
\usepackage{colortbl}

% Estilo do documento
\renewcommand{\familydefault}{\sfdefault}
\parskip=2pt
\parindent=0pt

% Cores para SQL
\definecolor{sqlkeyword}{RGB}{0,0,180}
\definecolor{sqlcomment}{RGB}{0,128,0}
\definecolor{sqlstring}{RGB}{163,21,21}
\definecolor{background}{RGB}{248,248,248}

% Linguagem SQL para listings
\lstdefinelanguage{SQL}{
    morekeywords={
        SELECT, FROM, WHERE, INSERT, INTO, VALUES, UPDATE, SET, DELETE, CREATE,
        TABLE, DROP, ALTER, ADD, INNER, LEFT, RIGHT, JOIN, ON, AS, DISTINCT, AND, OR, NOT, NULL, LIMIT, ORDER, BY, GROUP, HAVING, IN, LIKE, DESC
    },
    sensitive=false,
    morecomment=[l]{--},
    morestring=[b]',
}
\lstset{
    language=SQL,
    backgroundcolor=\color{background},
    basicstyle=\ttfamily\small,
    keywordstyle=\color{sqlkeyword}\bfseries,
    commentstyle=\color{sqlcomment}\itshape,
    stringstyle=\color{sqlstring},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    tabsize=2,
    columns=fullflexible,
}

% Linguagem para as expressões
\lstdefinelanguage{expr}{
    morekeywords={RecExpr, nodes},
    sensitive=false
}

\lstnewenvironment{exprlisting}{
    \lstset{
        language=expr,
        backgroundcolor=\color{white},
        basicstyle=\ttfamily\footnotesize,
        breaklines=true,
        frame=single,
        columns=fullflexible,
        showstringspaces=false
    }
}{}

% Definição simplificada da linguagem Rust para listings
\lstdefinelanguage{Rust}{
  keywords={
    abstract, alignof, as, async, await, become, box, break, const, continue,
    crate, do, dyn, else, enum, extern, false, final, fn, for, if, impl, in,
    let, loop, macro, match, mod, move, mut, offsetof, override, priv, proc,
    pub, pure, ref, return, self, sizeof, static, struct, super, trait, true,
    try, type, typeof, unsafe, unsized, use, virtual, where, while, yield,
    LazyLock, Vec, Rewrite, append, mut, rules
  },
  sensitive=true,
  comment=[l]{//},
  comment=[s]{/*}{*/},
  string=[b]"
}

% Caminho para imagens
\graphicspath{{images/}{img_general_analysis/}}

% Hyperref settings
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=black,
    urlcolor=black
}


\begin{document}

% Capa (se existir o ficheiro cover.tex)
\include{cover}
\makecover 
\pagenumbering{gobble}

\newgeometry{top=3cm,left=3cm,right=3cm,bottom=4cm}

% Abstract
\renewenvironment{abstract}
 {\par\noindent\textbf{\Large\abstractname}\par\bigskip}
 {}

\begin{abstract}
This report outlines the development of a project carried out as part of the \textbf{Project} curricular unit in the third year of the \textbf{Computer Science} degree at the \textbf{University of Minho}. \\

The main objective of the project is to perform an in-depth analysis of a SQL query optimizer, through the graphical representation of the various phases involved in the optimization process of each query. \\

To accomplish these objectives, and following the recommendation of our academic advisor, Professor \textbf{\href{https://jopereira.github.io}{José Orlando Pereira}}, we made use of the educational database system \textbf{Risinglight}, which includes a functional query optimizer available through its \textbf{\href{https://github.com/risinglightdb/risinglight}{GitHub repository}}. \\

The development process was structured into five main phases:
\begin{enumerate}
    \item Familiarization with the available tools and initial preparations: Cloning of the repository to \href{https://github.com/Blackparkd/risinglight}{our GitHub repository} to begin testing possibilities.
    \item Analysis of the problem and definition of the project's goals.
    \item Insertion of strategically placed print statements into the \texttt{optimizer.rs} file to trace the execution flow.
    \item Logging and saving of relevant execution data to external csv files.
    \item Generation of visual representations and graphs based on the collected data.
\end{enumerate}

This structured approach provided a deeper understanding of the inner workings of the optimizer and enabled a comprehensive analysis of its behavior across different stages.
\end{abstract}

\tableofcontents
\listoffigures
%\listoftables

\chapter{Introduction}\label{chap:intro}

\section{Objectives}
The main objectives of this project are:
\begin{itemize}
    \item To understand the inner workings of a SQL query optimizer through visual representation
    \item To analyze the different phases of the optimization process
    \item To identify patterns and bottlenecks in query optimization
    \item To visualize how different SQL queries are processed and optimized
    \item To provide insights that could lead to potential improvements in query optimization techniques
\end{itemize}

Our approach focused on instrumenting the Risinglight database system to collect detailed information about the optimization process, allowing us to generate visual representations that highlight key aspects of the optimizer's behavior.

\newpage
\section{Report structure}
To ease the reading and comprehension of this document, in this section will be explained the structure and content of each chapter.

\begin{itemize}
    \item \textbf{Capítulo ~\ref{chap:intro}} - Definição do Sistema, que consiste em: Introduzir e descrever a contextualização do projeto desenvolvido, assim como os objetivos a atingir com o mesmo.
    \item \textbf{Capítulo ~\ref{chap:analiseEspecificacao}} - Análise e Especificação: Descrever informalmente o problema, indicando o que se espera ser possível fazer.
    \item \textbf{Capítulo ~\ref{chap:concDesenho}} - Concepção/Desenho da Resolução: Expôr todas as estruturas de dados utilizadas, que, de uma forma prática, indicam que tipo de programas podem (ou não) ser criados nesta linguagem.
    \item \textbf{Capítulo ~\ref{chap:desegramatica}} - Desenho da Gramática: A gramática consiste numa demonstração mais organizada do que o compilador desenvolvido aceita.
    \item \textbf{Capítulo ~\ref{chap:sistemadesen}} - O Sistema Desenvolvido: Neste capítulo é apresentado o código de implementação do projeto, dividido por categorias, assim como uma breve explicação das suas funções.
    \item \textbf{Capítulo ~\ref{chap:codeteste}} - Codificação e Testes: Exemplificação de código reconhecido pelo nosso interpretador, e o seu respetivo código em Assembly.
    \item \textbf{Capítulo ~\ref{chap:concl}} - Conclusão: Contém as últimas impressões acerca do projeto desenvolvido e oportunidades de trabalho futuro.
    \item \textbf{Apêndice ~\ref{chap:lex}} - Código Lex: É apresentado o código inteiro do analisador léxico.
    \item \textbf{Apêndice ~\ref{chap:yacc}} - Código Yacc: É apresentado o código, agora como um todo, do Código Parser e Tradutor.
\end{itemize}

\chapter{Analysis and Specification}\label{chap:analiseEspecificacao}
\section{Informal Problem Description}\label{sec:descricaoProblema}

A SQL query optimizer is a critical component of any database management system. It transforms user-written queries into efficient execution plans, aiming to minimize resource usage and execution time. The optimization process involves several complex stages, including:
\begin{enumerate}
    \item Parsing the SQL query into a logical representation
    \item Applying transformation rules to generate alternative execution plans
    \item Estimating the cost of each plan
    \item Selecting the plan with the lowest estimated cost
\end{enumerate}

Understanding this process is challenging due to its complexity and the abstract nature of the operations performed. \\
Our project aims to make this process more tangible by analysing each stage and helping pinpoint the exact area which causes more inefficiency during query optimization.

To achieve this, we've instrumented the Risinglight query optimizer to capture detailed information about:
\begin{itemize}
    \item Expression groups generated during optimization
    \item Relational operators used in query plans
    \item Cost estimates for different execution alternatives
    \item Statistical metrics such as maximum, minimum, and average number of expressions
    \item Insight into a specific optimizing technique: merge
\end{itemize}

This data is then processed and displayed through histograms to provide further insight into the optimizer.

\section{Database Schema}

The Risinglight database system is designed to support the TPC-H benchmark, which is a widely used standard for evaluating the performance of database systems. The TPC-H schema consists of the following tables:
% Imagem das tabelas
\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\linewidth]{img/image.png}
    \caption{Database Schema}
    \label{fig:database_schema}
\end{figure}


\chapter{Optimization Stages and Rule Application}

The optimizer starts with the initial logical expression, which is a representation of the SQL query. This expression is then transformed through a series of stages, each applying a set of rules to optimize the query. \\
There are three main stages, each with its own set of rules and an unknown number of iterations, in order to target specific inefficiencies and improve the overall performance of the query execution plan. 

Throughout this report, we will refer to the initial state of the expression as \textbf{stage 0}, since it is the state before any optimization has been applied.
Below is a summary of the three stages and their respective rules:

\section{Stage 1: Pushdown Apply and Join Transformation}
In the first stage, the optimizer focuses on simplifying logical expressions and transforming subqueries into joins. The main goals are to:
\begin{itemize}
    \item \textbf{Simplify Boolean Expressions:} Rules such as \texttt{and\_rules} are used to simplify and normalize logical AND expressions, making the query plan easier to optimize in later stages.
    \item \textbf{Apply Always---Better Transformations:} The \texttt{always\_better\_rules} group contains general-purpose rules that are always beneficial, such as removing redundant operations or simplifying expressions.
    \item \textbf{Subquery to Join Conversion:} The \texttt{subquery\_rules} transform certain subqueries into equivalent join operations, enabling further optimizations and improving execution efficiency.
\end{itemize}

\section{Stage 2: Predicate and Projection Pushdown, Index Scan}
The second stage aims to reduce the amount of data processed by pushing filters and projections as close as possible to the data sources. The rules applied include:
\begin{itemize}
    \item \textbf{Expression Simplification:} General expression simplification rules (\texttt{expr::rules}) are applied to further normalize and reduce expressions.
    \item \textbf{Predicate Pushdown:} The \texttt{predicate\_pushdown\_rules} move filter conditions (WHERE clauses) down the query tree, so that irrelevant rows are filtered out early.
    \item \textbf{Projection Pushdown:} The \texttt{projection\_pushdown\_rules} ensure that only necessary columns are carried through each stage, minimizing data movement.
    \item \textbf{Index Scan Optimization:} The \texttt{index\_scan\_rules} attempt to replace full table scans with index scans when possible, improving query performance.
    \item \textbf{Always-Better Transformations:} These are re-applied to catch any new opportunities created by the previous rules.
\end{itemize}

\section{Stage 3: Join Reordering and Hash Join}
The third stage focuses on optimizing join operations, which are often the most expensive part of query execution. The rules in this stage include:
\begin{itemize}
    \item \textbf{Join Reordering:} The \texttt{join\_reorder\_rules} change the order in which joins are performed, aiming to minimize intermediate result sizes and overall cost.
    \item \textbf{Hash Join Optimization:} The \texttt{hash\_join\_rules} convert suitable joins into hash joins, which are more efficient for large datasets.
    \item \textbf{Predicate and Projection Pushdown:} These rules are re-applied to ensure that any new opportunities for pushdown created by join reordering are exploited.
    \item \textbf{Order Optimization:} The \texttt{order\_rules} optimize ORDER BY operations, potentially reducing sorting costs.
    \item \textbf{Boolean Simplification and Always-Better Transformations:} These are applied again to further refine the plan.
\end{itemize}

\subsection{Iterative Application}
Each group of rules is applied iteratively within its stage. The optimizer may revisit the same rule multiple times as the query plan evolves, ensuring that all possible optimizations are explored before moving to the next stage.


\chapter{Analysis} \label{chap:concDesenho}
The goal of the optimizer is mainly to reduce the cost of the execution of certain queries, this is accomplished through a series of optimization stages that eventually lead to a more efficient database. \\
Since our goal is to analyse this optimizer, we decided we should choose the TPC-H queries for which the cost decrease would be more significant. \\

\section{Initial Analysis}
We thought it would be helpful if we could see the cost for every query and compare it, before choosing the ones to further analyse. With this in mind and making use of the \textbf{Matplotlib library} in \textbf{Python}, we built a histogram, in which is displayed the cost for every stage the optimizer goes through. \\

The initial cost is omitted from this histogram, as it remains identical across all queries.
This value is defined as the maximum possible value that can be represented by a 32-bit floating point number and it is assigned using the line \textbf{\texttt{[let mut cost = f32::MAX]}}. This number has a value of approximately $3.4 \times 10^{38}$.

\begin{figure}[H]
    \includegraphics[width = 1.35\textwidth, height = 0.37\textheight, keepaspectratio]{img_cost_differential/cost_reduction_all_queries.png}
    \caption{Histogram of cost reduction}
    \label{fig:cost1}
\end{figure}

The histogram above does a good job at providing some insight as to what queries would be more advantageous to analyse with more depth. \\

To help understand the motivations behind this analysis, the table below has more detailed information about every query: initial and decreased costs alongside the stages.

\begin{figure}[H]
    \includegraphics[width = 1.45\textwidth, height = 0.40\textheight, keepaspectratio]{img_cost_differential/cost_reduction_table.png}
    \caption{Table of cost reduction}
    \label{fig:tab1}
\end{figure}

As we can see in the images above, the cost decrease through the 3 stages is more accentuated in the queries 2, 5, 7, 8 and 9. We will look at these 5 queries more closely to get some more information about what causes the cost to decrease as much.

\section{Selective Analysis}
In this section of the analysis, we will focus only on the queries mentioned in the previous section. We found the accentuated decrease in cost more interesting, and this section will help understand the reasons behind it. \\

While this analysis is only focused on the 5 queries mentioned, the remaining queries' analysis will be available on our GitHub repository - \textbf{\href{https://github.com/Blackparkd/risinglight/tree/main/src/planner/outputs/graphs}{Graphs folder}} for the reader to explore. \\

First, for clarity purposes, we will display the query in SQL, next the relational expression generated, and only afterwards will we proceed with the in-depth analysis.

\subsection{Query 2}
\begin{lstlisting}
select
    s_acctbal,
    s_name,
    n_name,
    p_partkey,
    p_mfgr,
    s_address,
    s_phone,
    s_comment
from
    part,
    supplier,
    partsupp,
    nation,
    region
where
    p_partkey = ps_partkey
    and s_suppkey = ps_suppkey
    and p_size = 15
    and p_type like '%BRASS'
    and s_nationkey = n_nationkey
    and n_regionkey = r_regionkey
    and r_name = 'EUROPE'
    and ps_supplycost = (
        select
            min(ps_supplycost)
        from
            partsupp,
            supplier,
            nation,
            region
        where
            p_partkey = ps_partkey
            and s_suppkey = ps_suppkey
            and s_nationkey = n_nationkey
            and n_regionkey = r_regionkey
            and r_name = 'EUROPE'
    )
order by
    s_acctbal desc,
    n_name,
    s_name,
    p_partkey
limit 100;
\end{lstlisting}

\subsubsection{Relational Expression}

The relational expression is a representation of the SQL query in a tree-like structure, where each node represents an operation or a relation. The expression is generated by the optimizer and serves as an intermediate representation before the final execution plan is created.

\textbf{Stage 0:}

\begin{exprlisting}
RecExpr { nodes: [Constant(Bool(true)), Column($1.2(1)), Column($1.1(1)), Column($1.0(1)), List([3, 2, 1]), Table($1), Scan([5, 4, 0]), Column($0.3(1)), Column($0.2(1)), Column($0.1(1)), Column($0.0(1)), List([10, 9, 8, 7]), Table($0), Scan([12, 11, 0]), Column($3.6(1)), Column($3.5(1)), Column($3.4(1)), Column($3.3(1)), Column($3.2(1)), Column($3.1(1)), Column($3.0(1)), List([20, 19, 18, 17, 16, 15, 14]), Table($3), Scan([22, 21, 0]), Column($4.4(1)), Column($4.3(1)), Column($4.2(1)), Column($4.1(1)), Column($4.0(1)), List([28, 27, 26, 25, 24]), Table($4), Scan([30, 29, 0]), Inner, Join([32, 0, 31, 23]), Join([32, 0, 33, 13]), Join([32, 0, 34, 6]), Constant(String(""EUROPE"")), Eq([2, 36]), Eq([8, 3]), Eq([17, 10]), Eq([20, 27]), Column($2.0), Eq([41, 28]), And([42, 40]), And([43, 39]), And([44, 38]), And([45, 37]), Filter([46, 35]), Min(25), List([48]), Agg([49, 47]), Filter([0, 50]), List([]), Order([52, 51]), Ref(48), List([54]), Proj([55, 53]), Constant(Int32(0)), Constant(Null), Limit([58, 57, 56]), Column($1.2), Column($1.1), Column($1.0), List([62, 61, 60]), Scan([5, 63, 0]), Column($0.3), Column($0.2), Column($0.1), Column($0.0), List([68, 67, 66, 65]), Scan([12, 69, 0]), Column($4.4), Column($4.3), Column($4.2), Column($4.1), Column($4.0), List([75, 74, 73, 72, 71]), Scan([30, 76, 0]), Column($3.6), Column($3.5), Column($3.4), Column($3.3), Column($3.2), Column($3.1), Column($3.0), List([84, 83, 82, 81, 80, 79, 78]), Scan([22, 85, 0]), Column($2.8), Column($2.7), Column($2.6), Column($2.5), Column($2.4), Column($2.3), Column($2.2), Column($2.1), List([41, 94, 93, 92, 91, 90, 89, 88, 87]), Table($2), Scan([96, 95, 0]), Join([32, 0, 97, 86]), Join([32, 0, 98, 77]), Join([32, 0, 99, 70]), Join([32, 0, 100, 64]), LeftOuter, Apply([102, 101, 59]), Eq([72, 54]), Eq([61, 36]), Eq([66, 62]), Eq([81, 68]), Constant(String(""\%BRASS"")), Like([91, 108]), Constant(Int32(15)), Eq([90, 110]), Eq([84, 74]), Eq([41, 75]), And([113, 112]), And([114, 111]), And([115, 109]), And([116, 107]), And([117, 106]), And([118, 105]), And([119, 104]), Filter([120, 103]), Filter([0, 121]), Desc(79), List([123, 67, 83, 41]), Order([124, 122]), List([79, 83, 67, 41, 93, 82, 80, 78]), Proj([126, 125]), Constant(Int32(100)), Limit([128, 57, 127])] }
\end{exprlisting}

One more reason to show the stage 0 expression is to show how the optimizer starts with a very complex expression (containing 129 nodes), and how it is gradually simplified throughout the stages. \\

\textbf{Stage 3:}

\begin{exprlisting}
RecExpr { nodes: [Constant(Bool(true)), Column($4.3(1)), Column($4.1(1)), Column($4.0(1)), List([3, 2, 1]), Table($4), Scan([5, 4, 0]), Column($3.3(1)), Column($3.0(1)), List([8, 7]), Table($3), Scan([10, 9, 0]), Column($0.2(1)), Column($0.0(1)), List([13, 12]), Table($0), Scan([15, 14, 0]), Column($1.1(1)), Column($1.0(1)), List([18, 17]), Table($1), Scan([20, 19, 0]), Constant(String(""EUROPE"")), Eq([17, 22]), Filter([23, 21]), List([18]), Proj([25, 24]), List([12]), Inner, HashJoin([28, 0, 25, 27, 26, 16]), List([13]), Proj([30, 29]), List([7]), HashJoin([28, 0, 30, 32, 31, 11]), List([8]), Proj([34, 33]), List([2]), HashJoin([28, 0, 34, 36, 35, 6]), List([3, 1]), Proj([38, 37]), Column($4.4), Column($4.3), Column($4.2), Column($4.1), Column($4.0), List([44, 43, 42, 41, 40]), Scan([5, 45, 0]), Column($3.6), Column($3.5), Column($3.4), Column($3.3), Column($3.2), Column($3.1), Column($3.0), List([53, 52, 51, 50, 49, 48, 47]), Scan([10, 54, 0]), Column($1.2), Column($1.1), Column($1.0), List([58, 57, 56]), Scan([20, 59, 0]), Eq([22, 57]), Filter([61, 60]), Column($0.3), Column($0.2), Column($0.1), Column($0.0), List([66, 65, 64, 63]), Scan([15, 67, 0]), List([58]), List([64]), HashJoin([28, 0, 70, 69, 68, 62]), List([50]), List([66]), HashJoin([28, 0, 73, 72, 71, 55]), List([43]), List([53]), HashJoin([28, 0, 76, 75, 74, 46]), List([53, 52, 51, 50, 49, 48, 47, 44, 43, 42, 41, 40, 66, 65, 64, 63, 58, 57, 56]), Proj([78, 77]), Column($2.8), Column($2.7), Column($2.6), Column($2.5), Column($2.4), Column($2.3), Column($2.2), Column($2.1), Column($2.0), List([88, 87, 86, 85, 84, 83, 82, 81, 80]), Table($2), Scan([90, 89, 0]), Constant(Int32(15)), Eq([83, 92]), Constant(String(""\%BRASS"")), Like([84, 94]), And([95, 93]), Filter([96, 91]), List([44]), List([88]), HashJoin([28, 0, 99, 98, 97, 79]), List([3]), LeftOuter, HashJoin([102, 0, 99, 98, 97, 79]), List([3]), Proj([106, 105]), Constant(Int32(100)), Limit([108, 57, 107])] }
\end{exprlisting}

As we can see, the optimizer managed to reduce the number of nodes from 129 in the initial expression to 109 in the final optimized form, while maintaining the semantic equivalence of the query. \\ This reduction in complexity is one of the factors that contributes to the overall cost decrease. \\

So now, a good question to ask is: \textbf{How does the optimizer manage to reduce the number of nodes?} \\

To help answering this question, it is helpful to analyse the histogram of the expression groups generated during the optimization process. 

\subsection{Expression Groups}

The expression groups are a representation of the different equivalent expressions generated during the optimization process. Each group contains expressions that are semantically equivalent but may differ in their internal representation. \\

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\linewidth]{img/expression_groups/q2_expression_groups.png}
    \caption{Query 2 - Expression Groups}
    \label{fig:exprgroups2}
\end{figure}

The histogram above displays the number of expression groups generated at each stage of the optimization process. Within each group, there may be an unknown number of equivalent expressions. \\

According to the \textbf{Cost Reduction} histogram (Figure \ref{fig:cost1}), the decrease in cost is more accentuated when we move from stage 2 to stage 3. This is the stage where the optimizer applies the join reordering and hash join rules. \\
This is also reflected in the histogram of expression groups (Figure \ref{fig:exprgroups2}), where we can see that the number of groups generated in stage 3 is significantly higher than in stage 2. \\
This gives the impression that the optimizer is, not only exploring a larger number of equivalent expressions in stage 3, but also being able to reduce the overall cost of the query. \\

This is a very interesting finding, as it suggests that the optimizer is capable of finding more efficient ways to represent the same operation, even when the number of equivalent expressions increases. \\

\subsection{Rule Application}

Given these new results, we can't help but wonder what exactly is happening during this stage that might trigger this increase in the number of expression groups. \\ Looking at the code in the \texttt{optimizer.rs} file, it's possible to learn exactly what rules are being applied during this stage. \\

The rules are defined in the \texttt{src/planner/rules} folder, and they are grouped into different categories. The rules that are applied during stage 3 are defined in the \texttt{src/planner/rules/plan.rs} file. \\

\begin{lstlisting}[language=Rust]
static STAGE3_RULES: LazyLock<Vec<Rewrite>> = LazyLock::new(|| {
    let mut rules = vec![];
    rules.append(&mut rules::expr::and_rules());
    rules.append(&mut rules::plan::always_better_rules());
    rules.append(&mut rules::plan::join_reorder_rules());
    rules.append(&mut rules::plan::hash_join_rules());
    rules.append(&mut rules::plan::predicate_pushdown_rules());
    rules.append(&mut rules::plan::projection_pushdown_rules());
    rules.append(&mut rules::order::order_rules());
    rules
});
\end{lstlisting}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\linewidth]{img/img_rule_mostpop/top_5_rules_stage3_q2.png}
    \caption{Query 2 - Rules in Stage 3}
    \label{fig:rules2}
\end{figure}

Looking at the most popular rules applied during stage 3 (Figure \ref{fig:rules2}), we can see that the most frequently applied rules are related to join operations and projection pushdown. This aligns with our understanding that stage 3 focuses heavily on optimizing joins and their ordering.

The high frequency of join-related rule applications explains the increase in expression groups, as each new join order or implementation strategy generates new equivalent expressions. Despite this increase in complexity, the optimizer successfully reduces the query's cost through these transformations.

A rule \texttt{inner-join-swap} being the most frequently applied shows how significant join reordering is for query optimization. Consider the basic associativity transformation: $(A \Join B) \Join C \Rightarrow A \Join (B \Join C)$. This seemingly simple change in join order can drastically impact performance. If table $B \Join C$ produces a smaller intermediate result than $A \Join B$, the right-side expression requires less memory and computation in subsequent joins. This transformation alone was applied 658 times during stage 3 optimization of Query 2, suggesting how extensively the optimizer explores the space of possible join orders to find the most efficient execution plan.


To better understand how these optimizations affect the query's execution plan, we can look at the merge operations performed during this stage.





\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{img_merges/q2_merge_counts.png}
    \caption{Query 2 - Merge operations}
    \label{fig:hist2}
\end{figure}

According to these 2 histograms, we can notice something interesting, yet, unexpected. \\
When we think of merge operations, it becomes intuitive to think that the number of groups of equivalent expressions would be decreasing, since the merge operations cuts down equivalent expressions to combine them into one, but, that is not the case.\\
As the merge operations become more frequent, so does the number of groups, which indicates that the optimizer is further expanding the possible ways to represent a certain operation, all while decreasing the general cost of the query.


\chapter{Final Conclusions and Future Work} \label{chap:concl}
This report describes our process of analyzing and visualizing the behavior of a SQL query optimizer through the instrumentation of the Risinglight database system. The project successfully achieved the objectives outlined in the introduction, providing valuable insights into the complex process of query optimization.

Our visual representations have made it possible to observe patterns and behaviors that would be difficult to discern from raw data alone. The analysis of expression groups, relational operators, and cost estimates has deepened our understanding of how different queries are optimized.

Some key findings from our analysis include:
\begin{itemize}
    \item The relationship between query complexity and the number of expression groups generated
    \item Common transformation patterns applied by the optimizer for specific query constructs
    \item Situations where cost estimates may not accurately reflect actual execution performance
\end{itemize}

While we believe that our project meets the requirements initially set, there is still room for future work. Potential extensions include:
\begin{itemize}
    \item Dynamic visualizations that show the step-by-step progression of the optimization process
    \item Interactive tools that allow users to modify queries and immediately see the impact on optimization
    \item Deeper analysis of specific optimization techniques, such as join reordering or predicate pushdown
    \item Integration with actual execution statistics to evaluate the effectiveness of optimization decisions
\end{itemize}

Overall, this project has provided valuable insights into query optimization and established a foundation for further research in this area.

\appendix

% Se precisares de glossário, ativa estas linhas:
% \usepackage{glossaries}
% \makeglossaries
% \printglossaries

\end{document}
